{
  "model_type": "improved",
  "dataset_type": "bonus",
  "num_epochs": 100,
  "batch_size": 32,
  "learning_rate": 0.001,
  "vocab_size": 110,
  "train_size": 800,
  "val_size": 200,
  "best_val_acc": 5.0,
  "final_train_acc": 97.75,
  "training_time": 170.4713852405548,
  "train_losses": [
    4.712298393249512,
    4.698586368560791,
    4.691405143737793,
    4.671521186828613,
    4.614420185089111,
    4.578764495849609,
    4.530347576141358,
    4.3228615951538085,
    4.126877527236939,
    4.063676385879517,
    3.932362270355225,
    3.751118106842041,
    3.653112211227417,
    3.510009698867798,
    3.469277067184448,
    3.351813669204712,
    3.2435508823394774,
    3.0845635604858397,
    2.8550397872924806,
    2.8458375549316406,
    2.6691856002807617,
    2.433547925949097,
    2.1168679189682007,
    1.8114938831329346,
    1.5942149543762207,
    1.3508858823776244,
    1.263112828731537,
    1.1214571356773377,
    0.9125306391716004,
    0.7507257318496704,
    0.6293596708774567,
    0.5846128523349762,
    0.5149531090259551,
    0.4967762231826782,
    0.3952812266349792,
    0.3680732429027557,
    0.34831890106201174,
    0.31481207311153414,
    0.32063030779361723,
    0.28789203822612763,
    0.2843377029895782,
    0.27906434297561644,
    0.24989144772291183,
    0.22627783626317977,
    0.23998921543359755,
    0.22149384766817093,
    0.22073224902153016,
    0.2261115774512291,
    0.22219110220670701,
    0.19761136740446092,
    0.22267360657453536,
    0.21369614213705063,
    0.22789535224437713,
    0.20789996206760405,
    0.19388148099184035,
    0.20593541562557222,
    0.18169671028852463,
    0.19825019866228102,
    0.20884998083114625,
    0.197959720492363,
    0.1851344373822212,
    0.18272113680839538,
    0.1920599888265133,
    0.16819264262914657,
    0.17021727025508882,
    0.18607264757156372,
    0.20113135546445846,
    0.19347123444080352,
    0.18121847212314607,
    0.18063977897167205,
    0.20806061506271362,
    0.1974289220571518,
    0.196762013733387,
    0.1941088020801544,
    0.18968588024377822,
    0.1745822075009346,
    0.19991683840751648,
    0.20283833861351014,
    0.1809026086330414,
    0.1790970656275749,
    0.19403957486152648,
    0.20243515729904174,
    0.17529191941022873,
    0.15921041786670684,
    0.19235278129577638,
    0.1738360494375229,
    0.21500070095062257,
    0.18152323633432388,
    0.1938785943388939,
    0.1848119553923607,
    0.18858854681253434,
    0.1762158140540123,
    0.1917377257347107,
    0.17833627462387086,
    0.18115566045045853,
    0.2024768963456154,
    0.20351774036884307,
    0.19079115182161333,
    0.20311700642108918,
    0.17474949061870576
  ],
  "train_accs": [
    0.625,
    1.0,
    1.125,
    1.25,
    1.25,
    1.625,
    3.25,
    3.125,
    4.0,
    5.0,
    4.5,
    6.625,
    6.75,
    10.125,
    10.75,
    11.25,
    13.375,
    14.5,
    19.75,
    19.625,
    23.875,
    26.0,
    38.625,
    44.125,
    48.5,
    58.25,
    62.625,
    66.0,
    72.875,
    79.5,
    83.125,
    83.75,
    87.125,
    87.75,
    90.25,
    92.5,
    92.875,
    93.125,
    92.25,
    94.25,
    94.875,
    93.875,
    95.125,
    95.375,
    95.25,
    96.5,
    96.0,
    96.125,
    95.0,
    97.0,
    95.625,
    95.875,
    96.5,
    96.75,
    97.625,
    97.0,
    97.125,
    97.25,
    96.125,
    96.375,
    97.125,
    97.0,
    96.875,
    97.875,
    98.375,
    96.75,
    97.0,
    96.125,
    96.0,
    96.75,
    95.75,
    96.625,
    97.0,
    97.0,
    96.5,
    97.25,
    96.375,
    96.25,
    96.875,
    97.375,
    96.125,
    96.125,
    97.625,
    98.125,
    96.25,
    97.625,
    95.5,
    97.25,
    96.125,
    96.875,
    97.0,
    97.125,
    96.375,
    96.5,
    97.375,
    96.125,
    96.625,
    96.75,
    96.0,
    97.75
  ],
  "val_losses": [
    4.70853499003819,
    4.7102532386779785,
    4.711574758802142,
    4.753922939300537,
    4.698785441262381,
    4.814216681889126,
    4.624965327126639,
    4.411109992436001,
    4.579068865094866,
    4.379963057381766,
    4.305552755083356,
    4.377014943531582,
    4.376084225518363,
    4.557644844055176,
    4.568374361310687,
    4.428042343684605,
    4.7285990715026855,
    4.587406022208078,
    5.159874439239502,
    5.678498949323382,
    5.542711189814976,
    5.808512960161481,
    5.522777761731829,
    6.116908958980015,
    6.135325023106167,
    6.4317523411342075,
    6.594483511788504,
    7.328498772212437,
    7.159377438681466,
    7.254050322941372,
    7.56418514251709,
    7.924393108912876,
    7.696426595960345,
    8.100490910666329,
    8.126315934317452,
    8.058932985578265,
    8.197735922677177,
    8.37810720716204,
    8.302966390337263,
    8.497372899736677,
    8.471984386444092,
    8.404783657618932,
    8.542394229343959,
    8.648795740945,
    8.576135226658412,
    8.702053615025111,
    8.672747271401542,
    8.748013155800956,
    8.762623923165458,
    8.744658402034215,
    8.739749567849296,
    8.718079090118408,
    8.65173169544765,
    8.760942459106445,
    8.793906756809779,
    8.776752131325859,
    8.87027611051287,
    8.808357988085065,
    8.8628751209804,
    8.866374628884452,
    8.838034561702184,
    8.833264827728271,
    8.850127628871373,
    8.845337050301689,
    8.848310266222272,
    8.893098763057164,
    8.863948208945137,
    8.861812455313546,
    8.93146698815482,
    8.907093320574079,
    8.862936905452184,
    8.839059148515974,
    8.896079472133092,
    8.888281685965401,
    8.87824283327375,
    8.880230358668737,
    8.885308606284005,
    8.906920160566058,
    8.933462619781494,
    8.902816023145403,
    8.89442491531372,
    8.897041865757533,
    8.862277916499547,
    8.880775792258126,
    8.86254004069737,
    8.874016284942627,
    8.873919078281947,
    8.901426860264369,
    8.945376873016357,
    8.911492211478096,
    8.88521317073277,
    8.883879661560059,
    8.899906975882393,
    8.822770936148506,
    8.889988354274205,
    8.871528489249092,
    8.896941321236747,
    8.832515444074359,
    8.791958332061768,
    8.909934861319405
  ],
  "val_accs": [
    1.0,
    0.0,
    0.0,
    0.5,
    1.5,
    0.0,
    1.0,
    3.0,
    1.0,
    2.5,
    3.0,
    3.5,
    2.5,
    4.5,
    3.0,
    5.0,
    3.0,
    4.0,
    2.5,
    2.0,
    3.5,
    4.0,
    4.5,
    4.0,
    2.5,
    5.0,
    3.5,
    4.0,
    5.0,
    2.0,
    4.5,
    2.0,
    3.0,
    2.0,
    3.0,
    2.0,
    3.5,
    3.0,
    2.5,
    3.5,
    3.5,
    2.5,
    2.5,
    3.0,
    2.5,
    3.5,
    3.0,
    3.0,
    3.0,
    3.0,
    3.5,
    3.5,
    3.0,
    3.5,
    3.0,
    3.0,
    4.0,
    3.5,
    3.5,
    3.5,
    3.5,
    3.0,
    3.5,
    3.5,
    3.5,
    3.0,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    4.0,
    2.5,
    3.5,
    3.0,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.0,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    3.5,
    4.0
  ],
  "challenges": [
    "Low accuracy at epoch 6: 1.00%",
    "Low accuracy at epoch 7: 3.00%",
    "Low accuracy at epoch 8: 1.00%",
    "Low accuracy at epoch 9: 2.50%",
    "Low accuracy at epoch 10: 3.00%",
    "Low accuracy at epoch 11: 3.50%",
    "Low accuracy at epoch 12: 2.50%",
    "Low accuracy at epoch 13: 4.50%",
    "Low accuracy at epoch 14: 3.00%",
    "Low accuracy at epoch 15: 5.00%",
    "Low accuracy at epoch 16: 3.00%",
    "Low accuracy at epoch 17: 4.00%",
    "Low accuracy at epoch 18: 2.50%",
    "Low accuracy at epoch 19: 2.00%",
    "Low accuracy at epoch 20: 3.50%",
    "Low accuracy at epoch 21: 4.00%",
    "Low accuracy at epoch 22: 4.50%",
    "Low accuracy at epoch 23: 4.00%",
    "Low accuracy at epoch 24: 2.50%",
    "Low accuracy at epoch 25: 5.00%",
    "Low accuracy at epoch 26: 3.50%",
    "Low accuracy at epoch 27: 4.00%",
    "Low accuracy at epoch 28: 5.00%",
    "Low accuracy at epoch 29: 2.00%",
    "Low accuracy at epoch 30: 4.50%",
    "Low accuracy at epoch 31: 2.00%",
    "Low accuracy at epoch 32: 3.00%",
    "Low accuracy at epoch 33: 2.00%",
    "Low accuracy at epoch 34: 3.00%",
    "Low accuracy at epoch 35: 2.00%",
    "Low accuracy at epoch 36: 3.50%",
    "Low accuracy at epoch 37: 3.00%",
    "Low accuracy at epoch 38: 2.50%",
    "Low accuracy at epoch 39: 3.50%",
    "Low accuracy at epoch 40: 3.50%",
    "Low accuracy at epoch 41: 2.50%",
    "Low accuracy at epoch 42: 2.50%",
    "Low accuracy at epoch 43: 3.00%",
    "Low accuracy at epoch 44: 2.50%",
    "Low accuracy at epoch 45: 3.50%",
    "Low accuracy at epoch 46: 3.00%",
    "Low accuracy at epoch 47: 3.00%",
    "Low accuracy at epoch 48: 3.00%",
    "Low accuracy at epoch 49: 3.00%",
    "Low accuracy at epoch 50: 3.50%",
    "Low accuracy at epoch 51: 3.50%",
    "Low accuracy at epoch 52: 3.00%",
    "Low accuracy at epoch 53: 3.50%",
    "Low accuracy at epoch 54: 3.00%",
    "Low accuracy at epoch 55: 3.00%",
    "Low accuracy at epoch 56: 4.00%",
    "Low accuracy at epoch 57: 3.50%",
    "Low accuracy at epoch 58: 3.50%",
    "Low accuracy at epoch 59: 3.50%",
    "Low accuracy at epoch 60: 3.50%",
    "Low accuracy at epoch 61: 3.00%",
    "Low accuracy at epoch 62: 3.50%",
    "Low accuracy at epoch 63: 3.50%",
    "Low accuracy at epoch 64: 3.50%",
    "Low accuracy at epoch 65: 3.00%",
    "Low accuracy at epoch 66: 3.50%",
    "Low accuracy at epoch 67: 3.50%",
    "Low accuracy at epoch 68: 3.50%",
    "Low accuracy at epoch 69: 3.50%",
    "Low accuracy at epoch 70: 3.50%",
    "Low accuracy at epoch 71: 4.00%",
    "Low accuracy at epoch 72: 2.50%",
    "Low accuracy at epoch 73: 3.50%",
    "Low accuracy at epoch 74: 3.00%",
    "Low accuracy at epoch 75: 3.50%",
    "Low accuracy at epoch 76: 3.50%",
    "Low accuracy at epoch 77: 3.50%",
    "Low accuracy at epoch 78: 3.50%",
    "Low accuracy at epoch 79: 3.50%",
    "Low accuracy at epoch 80: 3.50%",
    "Low accuracy at epoch 81: 3.50%",
    "Low accuracy at epoch 82: 3.50%",
    "Low accuracy at epoch 83: 3.50%",
    "Low accuracy at epoch 84: 3.00%",
    "Low accuracy at epoch 85: 3.50%",
    "Low accuracy at epoch 86: 3.50%",
    "Low accuracy at epoch 87: 3.50%",
    "Low accuracy at epoch 88: 3.50%",
    "Low accuracy at epoch 89: 3.50%",
    "Low accuracy at epoch 90: 3.50%",
    "Low accuracy at epoch 91: 3.50%",
    "Low accuracy at epoch 92: 3.50%",
    "Low accuracy at epoch 93: 3.50%",
    "Low accuracy at epoch 94: 3.50%",
    "Low accuracy at epoch 95: 3.50%",
    "Low accuracy at epoch 96: 3.50%",
    "Low accuracy at epoch 97: 3.50%",
    "Low accuracy at epoch 98: 3.50%",
    "Low accuracy at epoch 99: 4.00%"
  ],
  "solutions": [
    "Use conditional batch normalization for different conditions",
    "Multi-task learning with condition prediction",
    "Implement early stopping to prevent overfitting",
    "Use learning rate warmup"
  ]
}